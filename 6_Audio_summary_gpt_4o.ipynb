{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Audio Summary**\n"
      ],
      "metadata": {
        "id": "6CT_k2wHaO5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The audio summary is generated by sending the model the audio transcript. With just the audio, the model is likely to bias towards the audio content, and will miss the context provided by the presentations and visuals.\n",
        "\n",
        "\n",
        "**{audio}** input for GPT-4o isn't currently available but will be coming soon! For now, we use our existing whisper-1 model to process the audio\n"
      ],
      "metadata": {
        "id": "iDXKOxrfaVvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZCVTVCPaZ_6",
        "outputId": "358f408a-02c3-464e-e2cd-0feecca10261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/320.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPEN_AI_KEY=userdata.get('opeaikey4o')"
      ],
      "metadata": {
        "id": "qF9lzPINdLUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "## Set the API key and model name\n",
        "MODEL=\"gpt-4o\"\n",
        "client = OpenAI(api_key=OPEN_AI_KEY)"
      ],
      "metadata": {
        "id": "TNKy3a_3dPi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path=\"chatgpt_brief_intro_audio.mp3\""
      ],
      "metadata": {
        "id": "JSxM9LAhiOr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe the audio\n",
        "transcription = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=open(audio_path, \"rb\"),\n",
        ")\n",
        "## OPTIONAL: Uncomment the line below to print the transcription\n",
        "#print(\"Transcript: \", transcription.text + \"\\n\\n\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "    {\"role\": \"system\", \"content\":\"\"\"You are generating a transcript summary. Create a summary of the provided transcription. Respond in Markdown.\"\"\"},\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription.text}\"}\n",
        "        ],\n",
        "    }\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Z7wHSyafBr",
        "outputId": "9505f1a5-f902-4efa-ac03-f1f879bbe99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Summary\n",
            "\n",
            "In today's discussion, the focus is on ChatGPT, an AI-backed chatbot prototype developed by OpenAI. ChatGPT answers user queries conversationally, can write review codes, and solve math problems. It utilizes a neural network architecture and unsupervised learning to generate responses, allowing it to learn and respond without explicit instructions, making it a versatile tool for various conversational tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The audio summary is biased towards the content discussed during the speech, but comes out with much less structure than the video summary."
      ],
      "metadata": {
        "id": "0n1yELrKiY0V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Eb4bVBriYax"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}